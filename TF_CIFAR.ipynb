{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Week 9 - Tuning with Zalando dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sc-dataman/DL_NNets/blob/master/TF_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3-vodHpBMrF",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks and Deep Learning for Life Sciences and Health Applications - An introductory course about theoretical fundamentals, case studies and implementations in python and tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN8TpkNKBMrI",
        "colab_type": "text"
      },
      "source": [
        "(C) Umberto Michelucci 2018 - umberto.michelucci@gmail.com \n",
        "\n",
        "github repository: https://github.com/michelucci/zhaw-dlcourse-spring2019\n",
        "\n",
        "Spring Semester 2019"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19T_E1yKBMrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from random import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgxMB14WBMrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "def get_label_name(code):\n",
        "    if (code == 0):\n",
        "        return '(0) airplane'\n",
        "    elif (code == 1):\n",
        "        return '(1) automobile'\n",
        "    elif (code == 2):\n",
        "        return '(2) bird'\n",
        "    elif (code == 3):\n",
        "        return '(3) cat'\n",
        "    elif (code == 4):\n",
        "        return '(4) deer'\n",
        "    elif (code == 5):\n",
        "        return '(5) dog'\n",
        "    elif (code == 6):\n",
        "        return '(6) frog'\n",
        "    elif (code == 7):\n",
        "        return '(7) horse'\n",
        "    elif (code == 8):\n",
        "        return '(8) ship'\n",
        "    elif (code == 9):\n",
        "        return '(9) truck'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgxOPLEeBMrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = pd.read_csv('fashion-mnist_train.csv', header = 0)\n",
        "data_test = pd.read_csv('fashion-mnist_test.csv', header = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QREZEZwIR4uz",
        "colab_type": "code",
        "outputId": "68ea2970-6038-4a23-c4b9-7b80f2e5da59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "\n",
        "from keras.optimizers import adam\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "from keras.utils import np_utils # To transform labels in categorical\n",
        "from keras.datasets import cifar10 # To load the dataset\n",
        "\n",
        "from keras.constraints import maxnorm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('tf') # To tell TensorFlow the right order of dims\n",
        "\n",
        "# if using google colab and wants to upload or download files\n",
        "from google.colab import files\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "nClasses = 10\n",
        "y_train = np_utils.to_categorical(y_train,nClasses)\n",
        "y_test = np_utils.to_categorical(y_test,nClasses)\n",
        "\n",
        "# Normalizing pixel values to [0-1] range\n",
        "\n",
        "\n",
        "# Point 1: Transform images from (32,32,3) to 3072-dimensional vectors (32*32*3)\n",
        "\n",
        "X_train_vec = np.reshape(x_train,(50000,3072))\n",
        "X_test_vec = np.reshape(x_test,(10000,3072))\n",
        "\n",
        "\n",
        "X_train = X_train_vec.astype('float32')\n",
        "X_test = X_test_vec.astype('float32')\n",
        "\n",
        "y_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6R6Hy6EBMrR",
        "colab_type": "code",
        "outputId": "40454b37-544c-4e2c-9c98-068ebe2d666d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "X_train.shape\n",
        "\n",
        "train = X_train.transpose()\n",
        "labels_ = y_train.transpose()\n",
        " \n",
        "test = X_test.transpose()\n",
        "labels_test_ = y_test.transpose()\n",
        "  \n",
        "print(labels_.shape)\n",
        "print(train.shape)\n",
        "\n",
        "print(labels_test_.shape)\n",
        "print(test.shape)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 50000)\n",
            "(3072, 50000)\n",
            "(10, 10000)\n",
            "(3072, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56DhnGDwBMrW",
        "colab_type": "code",
        "outputId": "c22132dc-5ffd-4df4-e672-95db0179efad",
        "colab": {}
      },
      "source": [
        "data_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 785)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swia-EDzBMra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = data_train['label'].values.reshape(1, 60000)\n",
        "\n",
        "labels_ = np.zeros((60000, 10))\n",
        "labels_[np.arange(60000), labels] = 1\n",
        "labels_ = labels_.transpose()\n",
        "\n",
        "\n",
        "train = data_train.drop('label', axis=1).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aENEqwZSBMrc",
        "colab_type": "code",
        "outputId": "c7e89092-4943-4f0b-b459-2363a21c09fd",
        "colab": {}
      },
      "source": [
        "print(labels_.shape)\n",
        "print(train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 60000)\n",
            "(784, 60000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwGQUSr0BMrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_test = data_test['label'].values.reshape(1, 10000)\n",
        "\n",
        "labels_test_ = np.zeros((10000, 10))\n",
        "labels_test_[np.arange(10000), labels_test] = 1\n",
        "labels_test_ = labels_test_.transpose()\n",
        "\n",
        "\n",
        "test = data_test.drop('label', axis=1).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIk_0EqsBMrh",
        "colab_type": "code",
        "outputId": "3491bfdc-0637-4790-cebc-85c58209c588",
        "colab": {}
      },
      "source": [
        "labels_test_.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkQ_71bPBMrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = np.array(train)\n",
        "test = np.array(test)\n",
        "labels_ = np.array(labels_)\n",
        "labels_test_ = np.array(labels_test_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt8F4F5IBMrn",
        "colab_type": "code",
        "outputId": "a64e9833-8559-4cd0-9b3a-a3ee0c8ae781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "idx = 5\n",
        "plt.imshow(train[:,idx].reshape(28,28), cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
        "plt.axis(\"on\")\n",
        "plt.title(get_label_name(labels[:,idx]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-51c5eaced50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_label_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3072 into shape (28,28)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bikJ-5cBMrq",
        "colab_type": "text"
      },
      "source": [
        "# 1 layer with softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgL_h9LuBMrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_layers(number_neurons):\n",
        "    n_dim = 3072\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    \n",
        "    \n",
        "    # Number of neurons in the layers\n",
        "    n1 = number_neurons # Number of neurons in layer 1\n",
        "    n2 = number_neurons # Number of neurons in layer 2 \n",
        "    n3 = number_neurons\n",
        "    n4 = 10\n",
        "    #n5 = 10 # Neurons for the softmax function\n",
        "\n",
        "    cost_history = np.empty(shape=[0], dtype = float)\n",
        "    learning_rate = tf.placeholder(tf.float32, shape=())\n",
        "\n",
        "    stddev_f = 0.1\n",
        "\n",
        "    tf.set_random_seed(5)\n",
        "\n",
        "    X = tf.placeholder(tf.float32, [n_dim, None])\n",
        "    Y = tf.placeholder(tf.float32, [10, None])\n",
        "    W1 = tf.Variable(tf.random_normal([n1, n_dim], stddev=stddev_f)) \n",
        "    b1 = tf.Variable(tf.constant(0.0, shape = [n1,1]) )\n",
        "    W2 = tf.Variable(tf.random_normal([n2, n1], stddev=stddev_f)) \n",
        "    b2 = tf.Variable(tf.constant(0.0, shape = [n2,1])) \n",
        "    W3 = tf.Variable(tf.random_normal([n3,n2], stddev = stddev_f))\n",
        "    b3 = tf.Variable(tf.constant(0.0, shape = [n3,1]))\n",
        "    W4 = tf.Variable(tf.random_normal([n4,n3], stddev = stddev_f))\n",
        "    b4 = tf.Variable(tf.constant(0.0, shape = [n4,1]))\n",
        "    #W5 = tf.Variable(tf.truncated_normal([n5,n4], stddev = stddev_f))\n",
        "    #b5 = tf.Variable(tf.constant(stddev_f, shape = [n5,1]))\n",
        "\n",
        "    # Let's build our network...\n",
        "    Z1 = tf.nn.relu(tf.matmul(W1, X) + b1) # n1 x n_dim * n_dim x n_obs = n1 x n_obs\n",
        "    Z2 = tf.nn.relu(tf.matmul(W2, Z1) + b2) # n2 x n1 * n1 * n_obs = n2 x n_obs\n",
        "    Z3 = tf.nn.relu(tf.matmul(W3, Z2) + b3)\n",
        "    Z4 = tf.matmul(W4, Z3) + b4\n",
        "    #Z4 = tf.nn.relu(tf.matmul(W4, Z3) + b4)\n",
        "    #Z5 = tf.matmul(W5,Z4) + b5\n",
        "    y_ = tf.nn.softmax(Z4,0) # n2 x n_obs (10 x None)\n",
        "\n",
        "\n",
        "    cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    return optimizer, cost, y_, X, Y, learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6dzwoy-BMru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(number_neurons):\n",
        "    n_dim = 3072\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Number of neurons in the layers\n",
        "    n1 = number_neurons# Number of neurons in layer 1\n",
        "    n2 = 10 # Number of neurons in output layer \n",
        "\n",
        "    cost_history = np.empty(shape=[1], dtype = float)\n",
        "    learning_rate = tf.placeholder(tf.float32, shape=())\n",
        "\n",
        "    X = tf.placeholder(tf.float32, [n_dim, None])\n",
        "    Y = tf.placeholder(tf.float32, [10, None])\n",
        "    W1 = tf.Variable(tf.truncated_normal([n1, n_dim], stddev=.1)) \n",
        "    b1 = tf.Variable(tf.constant(0.1, shape = [n1,1]) )\n",
        "    W2 = tf.Variable(tf.truncated_normal([n2, n1], stddev=.1)) \n",
        "    b2 = tf.Variable(tf.constant(0.1, shape = [n2,1])) \n",
        "\n",
        "    # Let's build our network...\n",
        "    Z1 = tf.nn.relu(tf.matmul(W1, X) + b1) # n1 x n_dim * n_dim x n_obs = n1 x n_obs\n",
        "    Z2 = tf.matmul(W2, Z1) + b2 # n2 x n1 * n1 * n_obs = n2 x n_obs\n",
        "    y_ = tf.nn.softmax(Z2,0) # n2 x n_obs (10 x None)\n",
        "\n",
        "    cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    return optimizer, cost, y_, X, Y, learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg4zX3WmBMrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_layers(minibatch_size, training_epochs, features, classes, logging_step = 100, \n",
        "                 learning_r = 0.001, number_neurons = 15, debug = False):\n",
        "    \n",
        "    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    cost_history = []\n",
        "    for epoch in range(training_epochs+1):\n",
        "        for i in range(0, features.shape[1], minibatch_size):\n",
        "            X_train_mini = features[:,i:i + minibatch_size]\n",
        "            y_train_mini = classes[:,i:i + minibatch_size]\n",
        "\n",
        "            #if (i % 5000 == 0):\n",
        "            #    print('i = ',i)\n",
        "            \n",
        "            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})\n",
        "        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})\n",
        "        cost_history = np.append(cost_history, cost_)\n",
        "\n",
        "        if ((epoch % logging_step == 0) & debug):\n",
        "                print(\"Reached epoch\",epoch,\"cost J =\", cost_)\n",
        "                \n",
        "    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
        "    accuracy_train = accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess)\n",
        "    accuracy_test = accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess)\n",
        "                \n",
        "    return accuracy_train, accuracy_test, sess, cost_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z50zsjUtBMry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(minibatch_size, training_epochs, features, classes, logging_step = 100, learning_r = 0.001, number_neurons = 15):\n",
        "    \n",
        "    opt, c, y_, X, Y, learning_rate = build_model(number_neurons)\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    cost_history = []\n",
        "    for epoch in range(training_epochs+1):\n",
        "        for i in range(0, features.shape[1], minibatch_size):\n",
        "            X_train_mini = features[:,i:i + minibatch_size]\n",
        "            y_train_mini = classes[:,i:i + minibatch_size]\n",
        "\n",
        "            #if (i % 5000 == 0):\n",
        "            #    print('i = ',i)\n",
        "            \n",
        "            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})\n",
        "        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})\n",
        "        cost_history = np.append(cost_history, cost_)\n",
        "\n",
        "        if (epoch % logging_step == 0):\n",
        "                print(\"Reached epoch\",epoch,\"cost J =\", cost_)\n",
        "                \n",
        "    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
        "    accuracy_train = accuracy.eval({X: train, Y: labels_, learning_rate: learning_r}, session = sess)\n",
        "    accuracy_test = accuracy.eval({X: test, Y: labels_test_, learning_rate: learning_r}, session = sess)\n",
        "                \n",
        "    return accuracy_train, accuracy_test, sess, cost_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8CXHlgYBMr1",
        "colab_type": "code",
        "outputId": "9b85518a-9da7-444b-85c2-2b8babe89380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%time\n",
        "acc_train, acc_test, sess, cost_history = model(minibatch_size = 50, \n",
        "                              training_epochs = 50, \n",
        "                              features = train, \n",
        "                              classes = labels_, \n",
        "                              logging_step = 10,\n",
        "                              learning_r = 0.001,\n",
        "                              number_neurons = 15)\n",
        "\n",
        "print(acc_train)\n",
        "print(acc_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reached epoch 0 cost J = 0.3498002\n",
            "Reached epoch 10 cost J = 0.30307004\n",
            "Reached epoch 20 cost J = 0.29036918\n",
            "Reached epoch 30 cost J = 0.28387168\n",
            "Reached epoch 40 cost J = 0.27958515\n",
            "Reached epoch 50 cost J = 0.27633053\n",
            "0.32362\n",
            "0.334\n",
            "CPU times: user 4min 45s, sys: 8.14 s, total: 4min 53s\n",
            "Wall time: 4min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_5WykkOBMr6",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "Using grid search find the optimal number of neurons in the layer that gives you the best accuracy. With optimal is meant big enough but not too big. After a certain number, increasing the number will not help anymore. Try to finda  good balance between number of neurons and time required for training training.\n",
        "\n",
        "You can use a code similar to the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az1ciB7cBMr7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "00d9b5a9-8b1b-4c79-f41a-1449645ba94c"
      },
      "source": [
        "nn = [5,25 50]\n",
        "for nn_ in nn:\n",
        "    acc_train, acc_test, sess, cost_history = model(minibatch_size = 50, \n",
        "                              training_epochs = 50, \n",
        "                              features = train, \n",
        "                              classes = labels_, \n",
        "                              logging_step = 50,\n",
        "                              learning_r = 0.001,\n",
        "                              number_neurons = nn_)\n",
        "    print('Number:',nn_,'Acc. Train:', acc_train, 'Acc. Test', acc_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reached epoch 0 cost J = 0.32860315\n",
            "Reached epoch 50 cost J = 0.3105381\n",
            "Number: 1 Acc. Train: 0.17884 Acc. Test 0.1799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnSV-0hEBMr9",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 2 - Find the best learning rate and number of neurons with random and logarithmic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOlyU5fVBMr-",
        "colab_type": "text"
      },
      "source": [
        "Do hyperparameter tuning for\n",
        "\n",
        "- Learning rate\n",
        "- Number of neurons\n",
        "\n",
        "using random search and for the learning rate logarithimc search. \n",
        "\n",
        "You can use the following code as example.\n",
        "\n",
        "- Try the code with a different number of randomly selected values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAbJ3YQDBMr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neurons_ = np.random.randint(low=35, high=60.0, size=(10))\n",
        "\n",
        "r = -np.random.random([10])*4.0\n",
        "\n",
        "learning_ = 10**r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNHevStvBMsA",
        "colab_type": "code",
        "outputId": "5a3981b6-d577-4268-8ce1-4f1d751d1857",
        "colab": {}
      },
      "source": [
        "learning_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.17655096e-04, 5.86204655e-02, 1.04542655e-02, 2.23468096e-04,\n",
              "       6.66085557e-01, 1.90703727e-01, 9.20111902e-03, 4.61827071e-01,\n",
              "       5.22271634e-02, 7.82464921e-03])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVOUNiQ3BMsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(neurons_)):\n",
        "    acc_train, acc_test, sess, cost_history = model_layers(minibatch_size = 50, \n",
        "                              training_epochs = 50, \n",
        "                              features = train, \n",
        "                              classes = labels_, \n",
        "                              logging_step = 50,\n",
        "                              learning_r = learning_[i],\n",
        "                              number_neurons = neurons_[i], debug = False)\n",
        "    print('Number:',neurons_[i],'learning', learning_[i], 'Acc. Train:', acc_train, 'Acc. Test', acc_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwiac6YbBMsF",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 3 - Optimise learning rate, number of neurons and mini-batch size (with random search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtknV-JFBMsF",
        "colab_type": "text"
      },
      "source": [
        "Do hyperparameter tuning for\n",
        "\n",
        "- Learning rate\n",
        "- Number of neurons\n",
        "- mini-batch size\n",
        "\n",
        "using random search and for the learning rate logarithimc search. \n",
        "\n",
        "You can use the following code as example.\n",
        "\n",
        "- Try the code with a different number of randomly selected values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlXtctzfBMsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neurons_ = np.random.randint(low=35, high=60.0, size=(5))\n",
        "\n",
        "r = -np.random.random([10])*4.0\n",
        "\n",
        "learning_ = 10**r\n",
        "\n",
        "mb_size_ = np.random.randint(low=20, high=80, size = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxawFV0XBMsH",
        "colab_type": "code",
        "outputId": "8b0b0b0f-10f9-453b-b29d-ef883324de53",
        "colab": {}
      },
      "source": [
        "mb_size_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([66, 40, 70, 71, 51])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SZg9BbjBMsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(neurons_)):\n",
        "    #print('Number:',neurons_[i],'learning', learning_[i])\n",
        "    acc_train, acc_test, sess, cost_history = model_layers(minibatch_size = mb_size_[i], \n",
        "                              training_epochs = 50, \n",
        "                              features = train, \n",
        "                              classes = labels_, \n",
        "                              logging_step = 50,\n",
        "                              learning_r = learning_[i],\n",
        "                              number_neurons = neurons_[i], debug = False)\n",
        "    print('Number:',neurons_[i],'learning', learning_[i], 'mb size',mb_size_[i],\n",
        "          'Acc. Train:', acc_train, 'Acc. Test', acc_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC2dxZ6aBMsN",
        "colab_type": "text"
      },
      "source": [
        "# Let's test it with Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN2wieu_BMsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_layers(number_neurons):\n",
        "    n_dim = 784\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    \n",
        "    \n",
        "    # Number of neurons in the layers\n",
        "    n1 = number_neurons # Number of neurons in layer 1\n",
        "    n2 = number_neurons # Number of neurons in layer 2 \n",
        "    n3 = number_neurons\n",
        "    n4 = 10\n",
        "\n",
        "    cost_history = np.empty(shape=[0], dtype = float)\n",
        "    learning_rate = tf.placeholder(tf.float32, shape=())\n",
        "\n",
        "    stddev_f = 0.1\n",
        "\n",
        "    tf.set_random_seed(5)\n",
        "\n",
        "    X = tf.placeholder(tf.float32, [n_dim, None])\n",
        "    Y = tf.placeholder(tf.float32, [10, None])\n",
        "    W1 = tf.Variable(tf.random_normal([n1, n_dim], stddev=stddev_f)) \n",
        "    b1 = tf.Variable(tf.constant(0.0, shape = [n1,1]) )\n",
        "    W2 = tf.Variable(tf.random_normal([n2, n1], stddev=stddev_f)) \n",
        "    b2 = tf.Variable(tf.constant(0.0, shape = [n2,1])) \n",
        "    W3 = tf.Variable(tf.random_normal([n3,n2], stddev = stddev_f))\n",
        "    b3 = tf.Variable(tf.constant(0.0, shape = [n3,1]))\n",
        "    W4 = tf.Variable(tf.random_normal([n4,n3], stddev = stddev_f))\n",
        "    b4 = tf.Variable(tf.constant(0.0, shape = [n4,1]))\n",
        "    #W5 = tf.Variable(tf.truncated_normal([n5,n4], stddev = stddev_f))\n",
        "    #b5 = tf.Variable(tf.constant(stddev_f, shape = [n5,1]))\n",
        "\n",
        "    # Let's build our network...\n",
        "    Z1 = tf.nn.relu(tf.matmul(W1, X) + b1) # n1 x n_dim * n_dim x n_obs = n1 x n_obs\n",
        "    Z2 = tf.nn.relu(tf.matmul(W2, Z1) + b2) # n2 x n1 * n1 * n_obs = n2 x n_obs\n",
        "    Z3 = tf.nn.relu(tf.matmul(W3, Z2) + b3)\n",
        "    Z4 = tf.matmul(W4, Z3) + b4\n",
        "    #Z4 = tf.nn.relu(tf.matmul(W4, Z3) + b4)\n",
        "    #Z5 = tf.matmul(W5,Z4) + b5\n",
        "    y_ = tf.nn.softmax(Z4,0) # n2 x n_obs (10 x None)\n",
        "\n",
        "\n",
        "    cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))\n",
        "    optimizer = optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate, \n",
        "                                                   beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8).minimize(cost)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    return optimizer, cost, y_, X, Y, learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GaGWbuoBMsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_layers(minibatch_size, training_epochs, features, classes, logging_step = 100, \n",
        "                 learning_r = 0.001, number_neurons = 15, debug = False):\n",
        "    \n",
        "    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    cost_history = []\n",
        "    for epoch in range(training_epochs+1):\n",
        "        for i in range(0, features.shape[1], minibatch_size):\n",
        "            X_train_mini = features[:,i:i + minibatch_size]\n",
        "            y_train_mini = classes[:,i:i + minibatch_size]\n",
        "\n",
        "            #if (i % 5000 == 0):\n",
        "            #    print('i = ',i)\n",
        "            \n",
        "            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})\n",
        "        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})\n",
        "        cost_history = np.append(cost_history, cost_)\n",
        "\n",
        "        if ((epoch % logging_step == 0) & debug):\n",
        "                print(\"Reached epoch\",epoch,\"cost J =\", cost_)\n",
        "                \n",
        "    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
        "    accuracy_train = accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess)\n",
        "    accuracy_test = accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess)\n",
        "                \n",
        "    return accuracy_train, accuracy_test, sess, cost_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_aYruXmBMsR",
        "colab_type": "text"
      },
      "source": [
        "# Complete hyperparameter tuning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-SprUh_BMsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neurons_ = np.random.randint(low=35, high=60.0, size=(20))\n",
        "\n",
        "r = -np.random.random([20])*(6-5)-5\n",
        "\n",
        "learning_ = 10**r\n",
        "\n",
        "mb_size_ = np.random.randint(low=20, high=80, size = 20)\n",
        "\n",
        "epochs_ = np.random.randint(low = 40, high = 100, size = (20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqiw2O4LBMsU",
        "colab_type": "code",
        "outputId": "e4f49a60-c2b2-4841-fa6e-8dbe315119b0",
        "colab": {}
      },
      "source": [
        "for i in range(len(neurons_)):\n",
        "    #print('Number:',neurons_[i],'learning', learning_[i])\n",
        "    acc_train, acc_test, sess, cost_history = model_layers(minibatch_size = mb_size_[i], \n",
        "                              training_epochs = epochs_[i], \n",
        "                              features = train, \n",
        "                              classes = labels_, \n",
        "                              logging_step = 10,\n",
        "                              learning_r = learning_[i],\n",
        "                              number_neurons = neurons_[i], debug = False)\n",
        "    print('epochs:', epochs_[i], 'Number:',neurons_[i],'learning', learning_[i], 'mb size',mb_size_[i],\n",
        "          'Acc. Train:', acc_train, 'Acc. Test', acc_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs: 65 Number: 46 learning 3.75743361986e-06 mb size 20 Acc. Train: 0.838683 Acc. Test 0.8407\n",
            "epochs: 54 Number: 37 learning 7.12390609733e-06 mb size 56 Acc. Train: 0.8246 Acc. Test 0.8241\n",
            "epochs: 42 Number: 37 learning 2.14766290607e-06 mb size 45 Acc. Train: 0.712933 Acc. Test 0.7131\n",
            "epochs: 70 Number: 39 learning 5.59340704666e-06 mb size 76 Acc. Train: 0.814483 Acc. Test 0.8107\n",
            "epochs: 83 Number: 50 learning 8.14470677233e-06 mb size 32 Acc. Train: 0.1 Acc. Test 0.1\n",
            "epochs: 60 Number: 44 learning 3.97668096387e-06 mb size 58 Acc. Train: 0.8123 Acc. Test 0.815\n",
            "epochs: 52 Number: 35 learning 2.11130468914e-06 mb size 78 Acc. Train: 0.691283 Acc. Test 0.6906\n",
            "epochs: 70 Number: 43 learning 4.94147290757e-06 mb size 32 Acc. Train: 0.837283 Acc. Test 0.8374\n",
            "epochs: 70 Number: 44 learning 8.09986058619e-06 mb size 59 Acc. Train: 0.847767 Acc. Test 0.8477\n",
            "epochs: 46 Number: 35 learning 3.28760183015e-06 mb size 37 Acc. Train: 0.779567 Acc. Test 0.777\n",
            "epochs: 51 Number: 46 learning 3.11646009783e-06 mb size 41 Acc. Train: 0.805367 Acc. Test 0.8057\n",
            "epochs: 96 Number: 58 learning 2.05750793342e-06 mb size 49 Acc. Train: 0.820183 Acc. Test 0.8206\n",
            "epochs: 61 Number: 54 learning 1.30537893914e-06 mb size 38 Acc. Train: 0.77015 Acc. Test 0.768\n",
            "epochs: 72 Number: 45 learning 2.88601092474e-06 mb size 52 Acc. Train: 0.8057 Acc. Test 0.8053\n",
            "epochs: 94 Number: 52 learning 5.31016599e-06 mb size 37 Acc. Train: 0.1 Acc. Test 0.1\n",
            "epochs: 84 Number: 54 learning 2.97351049253e-06 mb size 43 Acc. Train: 0.832483 Acc. Test 0.831\n",
            "epochs: 42 Number: 36 learning 8.22904299748e-06 mb size 25 Acc. Train: 0.836567 Acc. Test 0.8364\n",
            "epochs: 70 Number: 50 learning 2.24455609222e-06 mb size 34 Acc. Train: 0.81065 Acc. Test 0.8111\n",
            "epochs: 93 Number: 37 learning 2.90045439769e-06 mb size 28 Acc. Train: 0.82645 Acc. Test 0.8278\n",
            "epochs: 82 Number: 36 learning 6.71104798403e-06 mb size 72 Acc. Train: 0.8331 Acc. Test 0.8329\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}