{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyzL0xlnEiVW+42GVz248h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sc-dataman/DL_NNets/blob/master/mar22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu9Wp7pJbVDP",
        "colab_type": "code",
        "outputId": "f31ab512-0086-4f67-83e6-4642d08712ab",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "\n",
        "\n",
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "#df = pd.read_csv('NSE-TATAGLOBAL(1).csv')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8055fb79-3930-4d99-82a2-3e11d53a452f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8055fb79-3930-4d99-82a2-3e11d53a452f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Google_Stock_Price_Train.csv to Google_Stock_Price_Train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH4VUFWEMYk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "06e14ff8-9a71-4a41-ed4a-067fd764abae"
      },
      "source": [
        "\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['Google_Stock_Price_Train.csv']))\n",
        "df.head(5)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/3/2012</td>\n",
              "      <td>325.25</td>\n",
              "      <td>332.83</td>\n",
              "      <td>324.97</td>\n",
              "      <td>663.59</td>\n",
              "      <td>7,380,500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/4/2012</td>\n",
              "      <td>331.27</td>\n",
              "      <td>333.87</td>\n",
              "      <td>329.08</td>\n",
              "      <td>666.45</td>\n",
              "      <td>5,749,400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/5/2012</td>\n",
              "      <td>329.83</td>\n",
              "      <td>330.75</td>\n",
              "      <td>326.89</td>\n",
              "      <td>657.21</td>\n",
              "      <td>6,590,300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/6/2012</td>\n",
              "      <td>328.34</td>\n",
              "      <td>328.77</td>\n",
              "      <td>323.68</td>\n",
              "      <td>648.24</td>\n",
              "      <td>5,405,900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1/9/2012</td>\n",
              "      <td>322.04</td>\n",
              "      <td>322.29</td>\n",
              "      <td>309.46</td>\n",
              "      <td>620.76</td>\n",
              "      <td>11,688,800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Date    Open    High     Low   Close      Volume\n",
              "0  1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
              "1  1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
              "2  1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
              "3  1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
              "4  1/9/2012  322.04  322.29  309.46  620.76  11,688,800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R51uqjqsM1E1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "fe842107-3bfd-4356-9eac-b1721c18454d"
      },
      "source": [
        "# Importing the training set\n",
        "#dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')\n",
        "\n",
        "dataset_train =  pd.read_csv(io.BytesIO(uploaded['Google_Stock_Price_Train.csv']))\n",
        "\n",
        "dataset_train.head(10)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/3/2012</td>\n",
              "      <td>325.25</td>\n",
              "      <td>332.83</td>\n",
              "      <td>324.97</td>\n",
              "      <td>663.59</td>\n",
              "      <td>7,380,500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/4/2012</td>\n",
              "      <td>331.27</td>\n",
              "      <td>333.87</td>\n",
              "      <td>329.08</td>\n",
              "      <td>666.45</td>\n",
              "      <td>5,749,400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/5/2012</td>\n",
              "      <td>329.83</td>\n",
              "      <td>330.75</td>\n",
              "      <td>326.89</td>\n",
              "      <td>657.21</td>\n",
              "      <td>6,590,300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/6/2012</td>\n",
              "      <td>328.34</td>\n",
              "      <td>328.77</td>\n",
              "      <td>323.68</td>\n",
              "      <td>648.24</td>\n",
              "      <td>5,405,900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1/9/2012</td>\n",
              "      <td>322.04</td>\n",
              "      <td>322.29</td>\n",
              "      <td>309.46</td>\n",
              "      <td>620.76</td>\n",
              "      <td>11,688,800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1/10/2012</td>\n",
              "      <td>313.70</td>\n",
              "      <td>315.72</td>\n",
              "      <td>307.30</td>\n",
              "      <td>621.43</td>\n",
              "      <td>8,824,000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1/11/2012</td>\n",
              "      <td>310.59</td>\n",
              "      <td>313.52</td>\n",
              "      <td>309.40</td>\n",
              "      <td>624.25</td>\n",
              "      <td>4,817,800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1/12/2012</td>\n",
              "      <td>314.43</td>\n",
              "      <td>315.26</td>\n",
              "      <td>312.08</td>\n",
              "      <td>627.92</td>\n",
              "      <td>3,764,400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1/13/2012</td>\n",
              "      <td>311.96</td>\n",
              "      <td>312.30</td>\n",
              "      <td>309.37</td>\n",
              "      <td>623.28</td>\n",
              "      <td>4,631,800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1/17/2012</td>\n",
              "      <td>314.81</td>\n",
              "      <td>314.81</td>\n",
              "      <td>311.67</td>\n",
              "      <td>626.86</td>\n",
              "      <td>3,832,800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Date    Open    High     Low   Close      Volume\n",
              "0   1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
              "1   1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
              "2   1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
              "3   1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
              "4   1/9/2012  322.04  322.29  309.46  620.76  11,688,800\n",
              "5  1/10/2012  313.70  315.72  307.30  621.43   8,824,000\n",
              "6  1/11/2012  310.59  313.52  309.40  624.25   4,817,800\n",
              "7  1/12/2012  314.43  315.26  312.08  627.92   3,764,400\n",
              "8  1/13/2012  311.96  312.30  309.37  623.28   4,631,800\n",
              "9  1/17/2012  314.81  314.81  311.67  626.86   3,832,800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5suAHcuNp02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "545825a3-710f-4893-91ba-9ee624395336"
      },
      "source": [
        "training_set = dataset_train.iloc[:, 1:2].values #.values is used for creating a numpy array\n",
        "\n",
        "training_set"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[325.25],\n",
              "       [331.27],\n",
              "       [329.83],\n",
              "       ...,\n",
              "       [793.7 ],\n",
              "       [783.33],\n",
              "       [782.75]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpuq5LiUI_Tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1)) #default feature range\n",
        "\"\"\"We have to take new variable and apply fit_transform method from minmax scaler class. Here fit means it is going to\n",
        "get the min and max of the input data so that it could apply values to the Normalisation formulae. And from that transform method\n",
        "it is going to compute scaled stock prices for each of the stock prices from the training set according to the Normalisation Formulae\"\"\"\n",
        "training_set_scaled = sc.fit_transform(training_set) \n",
        "\n",
        "# Creating a data structure with 60 timesteps and 1 output\n",
        "\"\"\"60 timesteps means that for each time t rnn is going to look at 60 stock prices before time t that is stock prices between 60 days before time t and time t\n",
        "and based on the trends and correlation it is capturing during the 60 periods time steps it is going to predict the next output.\"\"\"\n",
        "X_train = [] # inputs to the NN\n",
        "y_train = [] # output to the NN\n",
        "for i in range(60, 1258): #to populate X_train and y_train\n",
        "    X_train.append(training_set_scaled[i-60:i, 0])\n",
        "    y_train.append(training_set_scaled[i, 0])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "# Reshaping\n",
        "#Here we are getting 3d structure for X_train by adding third dimension as indicator.\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymvgKxDfJDtC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a012f18-ade8-401c-938b-e1261cc21f96"
      },
      "source": [
        "\n",
        "# Part 2 - Building the RNN using stacked LSTMs\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "\n",
        "\"\"\"Initialising the RNN as a sequence of layers as opposed to computational graphs\n",
        "here regressor is used because we are predicting continous values.\"\"\"\n",
        "regressor = Sequential()\n",
        "\n",
        "\"\"\"Adding the first LSTM layer and some Dropout regularisation\n",
        "  regressor is an object to sequential class. Sequential class contains add method which is used here.\n",
        "  Then we add lstm layer using lstm class. \n",
        "  It contains three arguments: 1. No of Units/neurons 2.To create stacked(more than 1 layer) LSTM we have to set return_sequences = True\n",
        "  3. input_shape contains three dimensions no of observations, no of timesteps and no of indicators, here we have taken last two dimensions because first one will\n",
        "  be automatically considered.\"\"\"\n",
        "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
        "#here we are using dropout regularization in order to prevent overfitting in the model.\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding a second LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding a third LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding a fourth LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding the output layer\n",
        "regressor.add(Dense(units = 1))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')#here loss is mse because we are doing regression here\n",
        "\n",
        "# Fitting the RNN to the Training set\n",
        "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
        "\"\"\"Since here our dataset is less that's why we have set our epochs = 100, We can also increase no of epochs but it might \n",
        "leads to overfitting which is bad for our model.\"\"\"\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1198/1198 [==============================] - 10s 8ms/step - loss: 0.0469\n",
            "Epoch 2/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0072\n",
            "Epoch 3/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0059\n",
            "Epoch 4/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0054\n",
            "Epoch 5/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0058\n",
            "Epoch 6/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0046\n",
            "Epoch 7/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0045\n",
            "Epoch 8/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0043\n",
            "Epoch 9/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0041\n",
            "Epoch 10/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0052\n",
            "Epoch 11/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0043\n",
            "Epoch 12/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0040\n",
            "Epoch 13/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0039\n",
            "Epoch 14/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0038\n",
            "Epoch 15/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0038\n",
            "Epoch 16/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0038\n",
            "Epoch 17/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0039\n",
            "Epoch 18/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0038\n",
            "Epoch 19/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0039\n",
            "Epoch 20/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0039\n",
            "Epoch 21/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0035\n",
            "Epoch 22/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0033\n",
            "Epoch 23/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0041\n",
            "Epoch 24/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0033\n",
            "Epoch 25/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0030\n",
            "Epoch 26/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0031\n",
            "Epoch 27/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0032\n",
            "Epoch 28/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0030\n",
            "Epoch 29/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0032\n",
            "Epoch 30/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0035\n",
            "Epoch 31/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0030\n",
            "Epoch 32/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0031\n",
            "Epoch 33/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0036\n",
            "Epoch 34/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0026\n",
            "Epoch 35/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0030\n",
            "Epoch 36/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0030\n",
            "Epoch 37/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 38/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0028\n",
            "Epoch 39/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0026\n",
            "Epoch 40/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 41/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0026\n",
            "Epoch 42/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0030\n",
            "Epoch 43/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0024\n",
            "Epoch 44/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0028\n",
            "Epoch 45/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0025\n",
            "Epoch 46/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0025\n",
            "Epoch 47/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0026\n",
            "Epoch 48/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 49/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 50/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0025\n",
            "Epoch 51/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0024\n",
            "Epoch 52/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 53/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0025\n",
            "Epoch 54/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 55/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 56/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 57/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 58/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0022\n",
            "Epoch 59/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 60/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 61/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 62/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 63/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 64/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 65/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 66/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 67/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 68/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 69/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 70/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 71/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 72/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 73/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0022\n",
            "Epoch 74/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 75/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 76/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 77/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 78/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 79/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 80/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 81/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 82/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 83/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 84/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 85/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 86/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 87/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 88/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 89/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 90/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 91/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 92/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 93/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 94/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 95/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 96/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 97/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 98/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 99/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 100/100\n",
            "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Since here our dataset is less that's why we have set our epochs = 100, We can also increase no of epochs but it might \\nleads to overfitting which is bad for our model.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FepjRmQJISB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Part 3 - Making the predictions and visualising the results\n",
        "\n",
        "# Getting the real stock price of 2017\n",
        "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
        "real_stock_price = dataset_test.iloc[:, 1:2].values #converting it into numpy array using .values at end\n",
        "\n",
        "# Getting the predicted stock price of 2017\n",
        "\"\"\"Here to predict the stock price of particular day we have to take input as a stockprices of previous 60 days.\n",
        "And while predicting the output some of the stock prices in Jan 2017 have their inputs as the stock prices in test set.\n",
        "Therefore we have to concatenate both the dataset and forms the total dataset. \"\"\"\n",
        "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
        "#here we have not used iloc method from pandas to get inputs so here there are chances that we can face format problem so we have to reshape the inputs using reshape function\n",
        "inputs = inputs.reshape(-1,1)\n",
        "#scaling the inputs\n",
        "inputs = sc.transform(inputs)\n",
        "#loading the X_test list with values for predicting the Jan 2017 stock prices\n",
        "X_test = []\n",
        "for i in range(60, 80):\n",
        "    X_test.append(inputs[i-60:i, 0])\n",
        "X_test = np.array(X_test)\n",
        "#adding third dimension in X_test \n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "#using predict method for predicting values Using X_test as input values\n",
        "predicted_stock_price = regressor.predict(X_test)\n",
        "#inverse_transform method is used for obtaining unscaled/original values.\n",
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN_r12B4JP4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Visualising the results\n",
        "plt.plot(real_stock_price, color = 'black', label = 'Real Google Stock Price')\n",
        "plt.plot(predicted_stock_price, color = 'red', label = 'Predicted Google Stock Price')\n",
        "plt.title('Google Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Google Stock Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "Conclusion: So here we can see that our model can closely predict the trends which are quite similar to the trends in the Google stock real prices.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}